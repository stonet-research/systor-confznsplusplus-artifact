#! /bin/bash

set -e

SCRIPT_DIR=$(cd $(dirname "${BASH_SOURCE[0]}") && pwd)
cd ${SCRIPT_DIR}
source ../common.sh

# Input
if [ $# != 2 ]; then
    echo "Usage: $0 <ZNS device (e.g., nvme0n2)>  $1 <zones to finish (e.g., 300)>"
    exit 1
fi
DEV=$1
FINISH_SIZE=$2

# Device info
get_device_info ${DEV}

##################################
## fio options for specific ZNS ##
##################################
# Use 300 finish zones as with ~1 finish/sec and at 50% rate limit we can do 1 finish every 2 seconds
# and benchmarks try to run for 10 minutes (10*60/2) = 300 and peak write bandwidth is 1.2GB/s = ~1 zone so within 10
# minutes we can fill (10*60) = 600 and we have a total of 904 zones
# FINISH_SIZE=300
WRITE_ZONES=$(echo "${DEV_ZONES} - ${FINISH_SIZE}" | bc) # All but the fill zones
# our setup only has 20 cores and we have a polling thread for each job
# therefore we max to 9 write jobs, as there is 1 concurrent trim job
# issuing zone finish with also its own polling thread
WRITE_IODEPTH=(1 2 3 4 5 6 7) 
WRITE_FLOW=(100 99 95 90 75 50)

echo "#########################################################"
echo "###################### BENCH  SETUP #####################"
echo "#########################################################"
echo "---------------------------------------------------------"
printf 'FINISH_ZONES (Empty zones for finish): %18s\n' "${FINISH_SIZE}"
echo "---------------------------------------------------------"
printf 'WRITE_ZONES (Empty zones for writes): %19s\n' "${WRITE_ZONES}"
echo "---------------------------------------------------------"
printf 'WRITE_NUMJOBS: %42s\n' "${WRITE_IODEPTH[*]}"
echo "---------------------------------------------------------"
printf 'WRITE_RATE_FLOWS: %39s\n' "${WRITE_FLOW[*]}"
echo "---------------------------------------------------------"
echo ""

baseline_finish ${DEV_CHAR} ${FINISH_SIZE}

for write_iodepth in ${WRITE_IODEPTH[@]}; do
    for write_flow in ${WRITE_FLOW[@]}; do
        finish_flow=$((100 - ${write_flow}))
        WRITE_SIZE=$(echo "scale=0; ${WRITE_ZONES} / ${write_iodepth}" | bc)
        echo "Benchmarking WRITE_IODEPTH ${write_iodepth}: <WRITE_FLOW ${write_flow} - FINISH_LIMIT ${finish_flow}>"
        sudo nvme zns reset-zone /dev/${DEV_CHAR} -a

        if [[ ${finish_flow} -eq 0 ]]; then
            # With 0 finish flow we want to disable the concurrent finish workload
            CONCUR_WRITE_SIZE=$(echo "scale=0; ${WRITE_ZONES} / ${write_iodepth}" | bc)
            sudo env "DEV=${DEV_CHAR}" "CONCUR_WRITE_SIZE=${CONCUR_WRITE_SIZE}" "WRITE_IODEPTH=${write_iodepth}" \
                ${FIO} \
                    --output-format=json \
                    --output=${SCRIPT_DIR}/data/data-write_iod_${write_iodepth}-wflow_${write_flow}.json \
                    ${SCRIPT_DIR}/jobs/job-write.fio
        else
            CONCUR_WRITE_SIZE=$(echo "scale=0; ${WRITE_ZONES} / ${write_iodepth}" | bc)
            finish_rate_limit=$(echo "scale=2; ${finish_flow} / 100  * ${FINISH_BW}" | bc)
            # Write job has a ramp time because fio will directly issue 2 finishs and then realize it needs to rate limit, therefore we don't want to account this period in the write performance
            sudo env "DEV=${DEV}" "CONCUR_WRITE_SIZE=${CONCUR_WRITE_SIZE}" "WRITE_IODEPTH=${write_iodepth}" \
                "FINISH_SIZE=${FINISH_SIZE}" "FINISH_LIMIT=${finish_rate_limit}" ${FIO} \
                    --output-format=json \
                    --output=${SCRIPT_DIR}/data/data-write_iod_${write_iodepth}-wflow_${write_flow}.json \
                    ${SCRIPT_DIR}/jobs/job.fio
        fi
    done
done

